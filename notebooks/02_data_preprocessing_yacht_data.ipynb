{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2636cb1b-f7ef-40a8-9c11-cb764dbdb2e5",
      "metadata": {
        "id": "2636cb1b-f7ef-40a8-9c11-cb764dbdb2e5"
      },
      "source": [
        " 02 – DATA PREPROCESSING\n",
        "\n",
        "The purpose of this notebook is to clean and prepare the raw data so it’s suitable for analysis and modeling. This step includes loading the dataset, checking for issues like missing values or duplicates, and making sure the data types are correct. It also covers encoding categorical variables, scaling features, and preparing the train/test split."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13c99a5-488a-490a-9910-4cb52abad1f8",
      "metadata": {
        "id": "e13c99a5-488a-490a-9910-4cb52abad1f8"
      },
      "source": [
        "1. Load the Datasets\n",
        "   \n",
        "Instead of loading a single file, this step loads all CSV files located in the data/raw/ folder into a dictionary of pandas DataFrames. Each dataset is stored using its filename (without the .csv extension) as the key. This approach allows easy access and inspection of multiple datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "KsZ_JgrHC714",
        "outputId": "8c6dcd4c-7267-403c-b579-1a4e27a44162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KsZ_JgrHC714",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "577de4e1-8c62-40b6-9d93-ab0c405ac994",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "577de4e1-8c62-40b6-9d93-ab0c405ac994",
        "outputId": "82fb2068-35d6-446c-d4ae-d572ef00e9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded named_anchorages_v1_20191205 with shape (166508, 10)\n",
            "Loaded CVP_loitering_202411 with shape (684, 14)\n",
            "Loaded named_anchorages_v1_20181108 with shape (119748, 7)\n",
            "Loaded Weather-for-Boating-Activities with shape (1060, 6)\n",
            "Loaded CVP_ports_202411 with shape (1410, 14)\n",
            "Loaded boat_data with shape (9888, 10)\n",
            "Loaded CVP_encounters_202411 with shape (348, 14)\n",
            "Loaded sar_vessel_detections_pipev20231026_202410 with shape (268681, 10)\n",
            "Loaded named_anchorages_v2_20201104 with shape (166515, 10)\n",
            "Loaded boat_dataset with shape (10344, 38)\n",
            "Loaded Boats_No_Price_dataset with shape (936, 26)\n",
            "Loaded named_anchorages_v2_20221206 with shape (166482, 10)\n",
            "Loaded sar_vessel_detections_pipev3_202411 with shape (248247, 10)\n",
            "Loaded sar_vessel_detections_pipev3_202412 with shape (239081, 10)\n",
            "\n",
            "First file preview:        s2id        lat        lon     label sublabel     label_source iso3  \\\n",
            "0  3e4e429b  26.914042  52.220320   SHARJAH      NaN  top_destination  IRN   \n",
            "1  1a575de7  -7.715992  11.724560  BLOCK 17      NaN  top_destination  AGO   \n",
            "2  3fcf5295  29.642077  48.696705  KAZ IRAQ      NaN  top_destination  KWT   \n",
            "3  3fcf52bf  29.644148  48.701873  KAZ IRAQ      NaN  top_destination  KWT   \n",
            "4  3fcf52bd  29.639744  48.701769  UMM QASR      NaN  top_destination  KWT   \n",
            "\n",
            "   distance_from_shore_m  drift_radius at_dock  \n",
            "0                63000.0      0.056322   False  \n",
            "1               134000.0      0.111111   False  \n",
            "2                33000.0      0.162583   False  \n",
            "3                33000.0      0.161623   False  \n",
            "4                33000.0      0.149964   False  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-f3543a93a66b>:27: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  print(f\"\\nFirst file preview: {pd.read_csv(first_file, encoding='ISO-8859-1').head()}\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the raw data folder\n",
        "raw_data_path = '/content/drive/MyDrive/yacht-data-insights/data/raw/'\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "csv_files = glob.glob(os.path.join(raw_data_path, '*.csv'))\n",
        "\n",
        "# Create a dictionary of DataFrames with the filename (without extension) as the key\n",
        "dataframes = {}\n",
        "\n",
        "for file in csv_files:\n",
        "    name = os.path.splitext(os.path.basename(file))[0]  # filename without path or extension\n",
        "    try:\n",
        "        # Try reading the CSV with a specified encoding\n",
        "        df = pd.read_csv(file, encoding='ISO-8859-1', low_memory=False)\n",
        "        dataframes[name] = df\n",
        "        print(f\"Loaded {name} with shape {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {name}: {e}\")\n",
        "\n",
        "# Optionally, check the first file to confirm\n",
        "if csv_files:\n",
        "    first_file = csv_files[0]\n",
        "    print(f\"\\nFirst file preview: {pd.read_csv(first_file, encoding='ISO-8859-1').head()}\")\n",
        "else:\n",
        "    print(\"No CSV files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce4ae79-199d-4407-8a7c-7ff765c2c3b4",
      "metadata": {
        "id": "cce4ae79-199d-4407-8a7c-7ff765c2c3b4"
      },
      "source": [
        "2. Initial Checks: Data Types, Missing Values, and Duplicates\n",
        "This step checks for missing values, incorrect data types, and duplicate records — common issues that need to be addressed before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becd6270-e50d-4b55-aa5b-0d196057956f",
      "metadata": {
        "id": "becd6270-e50d-4b55-aa5b-0d196057956f"
      },
      "outputs": [],
      "source": [
        "# Overview of the dataset\n",
        "df.info()\n",
        "\n",
        "# Count missing values per column\n",
        "df.isnull().sum()\n",
        "\n",
        "# Check for duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c571d596-14c6-4fd6-b6bc-6b34954efad5",
      "metadata": {
        "id": "c571d596-14c6-4fd6-b6bc-6b34954efad5"
      },
      "source": [
        "3. Handle Missing Values\n",
        "Dropped rows with missing values in the target column, since they can’t be used for training. Other missing numerical values were filled using the median to avoid skewing from outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0808ef-67f0-47ec-8d30-744c3de61f61",
      "metadata": {
        "id": "3e0808ef-67f0-47ec-8d30-744c3de61f61"
      },
      "outputs": [],
      "source": [
        "# Drop rows where the target is missing\n",
        "df = df.dropna(subset=['target_column'])\n",
        "\n",
        "# Fill missing values in numerical columns\n",
        "df['numerical_column'] = df['numerical_column'].fillna(df['numerical_column'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ef14ca-e0a5-4ddf-83b8-5f82708e69b1",
      "metadata": {
        "id": "19ef14ca-e0a5-4ddf-83b8-5f82708e69b1"
      },
      "source": [
        "4. Fix Data Types\n",
        "Some columns were not in the correct format for processing. Converting them to the appropriate data types ensures compatibility with future transformations and modeling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14454a8-3da1-4087-b579-44578b3dbf97",
      "metadata": {
        "id": "d14454a8-3da1-4087-b579-44578b3dbf97"
      },
      "outputs": [],
      "source": [
        "# Convert values to numeric\n",
        "df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c331bd89-d996-45de-8a29-bdde2b724737",
      "metadata": {
        "id": "c331bd89-d996-45de-8a29-bdde2b724737"
      },
      "source": [
        "5. Remove Duplicates\n",
        "Removing duplicate rows to avoid bias and redundancy during analysis or training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255dc7e7-62e4-4182-a766-c7bfde6b1dd7",
      "metadata": {
        "id": "255dc7e7-62e4-4182-a766-c7bfde6b1dd7"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9b48fe-f4eb-4c49-8344-2611dd53d118",
      "metadata": {
        "id": "be9b48fe-f4eb-4c49-8344-2611dd53d118"
      },
      "source": [
        "6. Encode Categorical Variables\n",
        "Categorical columns were one-hot encoded to convert them into a numerical format suitable for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b6e2b0-8854-484a-b897-acd0b0d0af40",
      "metadata": {
        "id": "52b6e2b0-8854-484a-b897-acd0b0d0af40"
      },
      "outputs": [],
      "source": [
        "# One-hot encode selected categorical column\n",
        "df = pd.get_dummies(df, columns=['categorical_column'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf51003-4de7-4265-b3e6-d883ae0820b7",
      "metadata": {
        "id": "5bf51003-4de7-4265-b3e6-d883ae0820b7"
      },
      "source": [
        "7. Scale Numerical Features\n",
        "Numerical features were scaled using StandardScaler to ensure all features contribute equally to model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87b7108-3446-411a-8ebc-642c2b2e6a45",
      "metadata": {
        "id": "f87b7108-3446-411a-8ebc-642c2b2e6a45"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "df[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(df[['numerical_column1', 'numerical_column2']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8afe38b-f366-42ca-b011-d56d24454997",
      "metadata": {
        "id": "f8afe38b-f366-42ca-b011-d56d24454997"
      },
      "source": [
        "8. Split Dataset into Train/Test\n",
        "The cleaned and prepared dataset was split into training and testing sets to allow model validation on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f255ce-c742-49f3-a7af-a36c3ce471cb",
      "metadata": {
        "id": "38f255ce-c742-49f3-a7af-a36c3ce471cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Create training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}