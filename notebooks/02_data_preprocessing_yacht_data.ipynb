{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2636cb1b-f7ef-40a8-9c11-cb764dbdb2e5",
   "metadata": {},
   "source": [
    " 02 – DATA PREPROCESSING \n",
    " \n",
    "The purpose of this notebook is to clean and prepare the raw data so it’s suitable for analysis and modeling. This step includes loading the dataset, checking for issues like missing values or duplicates, and making sure the data types are correct. It also covers encoding categorical variables, scaling features, and preparing the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c99a5-488a-490a-9910-4cb52abad1f8",
   "metadata": {},
   "source": [
    "1. Load the Datasets\n",
    "   \n",
    "Instead of loading a single file, this step loads all CSV files located in the data/raw/ folder into a dictionary of pandas DataFrames. Each dataset is stored using its filename (without the .csv extension) as the key. This approach allows easy access and inspection of multiple datasets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577de4e1-8c62-40b6-9d93-ab0c405ac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the raw data folder\n",
    "raw_data_path = 'data/raw/'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(raw_data_path, '*.csv'))\n",
    "\n",
    "# Create a dictionary of DataFrames with the filename (without extension) as the key\n",
    "dataframes = {}\n",
    "\n",
    "# Function to load CSV files in chunks\n",
    "def load_csv_in_chunks(file_path, chunk_size=5000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk_list.append(chunk)\n",
    "    return pd.concat(chunk_list, axis=0)\n",
    "\n",
    "# Loop through all CSV files\n",
    "for file in csv_files:\n",
    "    name = os.path.splitext(os.path.basename(file))[0]  # filename without path or extension\n",
    "    df = load_csv_in_chunks(file, chunk_size=5000)\n",
    "    dataframes[name] = df\n",
    "    print(f\"Loaded {name} with shape {df.shape}\")\n",
    "\n",
    "    print(f\"Processing {file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4ae79-199d-4407-8a7c-7ff765c2c3b4",
   "metadata": {},
   "source": [
    "2. Initial Checks: Data Types, Missing Values, and Duplicates\n",
    "This step checks for missing values, incorrect data types, and duplicate records — common issues that need to be addressed before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd6270-e50d-4b55-aa5b-0d196057956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the dataset\n",
    "df.info()\n",
    "\n",
    "# Count missing values per column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571d596-14c6-4fd6-b6bc-6b34954efad5",
   "metadata": {},
   "source": [
    "3. Handle Missing Values\n",
    "Dropped rows with missing values in the target column, since they can’t be used for training. Other missing numerical values were filled using the median to avoid skewing from outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0808ef-67f0-47ec-8d30-744c3de61f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the target is missing\n",
    "df = df.dropna(subset=['target_column'])\n",
    "\n",
    "# Fill missing values in numerical columns\n",
    "df['numerical_column'] = df['numerical_column'].fillna(df['numerical_column'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef14ca-e0a5-4ddf-83b8-5f82708e69b1",
   "metadata": {},
   "source": [
    "4. Fix Data Types\n",
    "Some columns were not in the correct format for processing. Converting them to the appropriate data types ensures compatibility with future transformations and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14454a8-3da1-4087-b579-44578b3dbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to numeric\n",
    "df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331bd89-d996-45de-8a29-bdde2b724737",
   "metadata": {},
   "source": [
    "5. Remove Duplicates\n",
    "Removing duplicate rows to avoid bias and redundancy during analysis or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255dc7e7-62e4-4182-a766-c7bfde6b1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b48fe-f4eb-4c49-8344-2611dd53d118",
   "metadata": {},
   "source": [
    "6. Encode Categorical Variables\n",
    "Categorical columns were one-hot encoded to convert them into a numerical format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6e2b0-8854-484a-b897-acd0b0d0af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode selected categorical column\n",
    "df = pd.get_dummies(df, columns=['categorical_column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf51003-4de7-4265-b3e6-d883ae0820b7",
   "metadata": {},
   "source": [
    "7. Scale Numerical Features\n",
    "Numerical features were scaled using StandardScaler to ensure all features contribute equally to model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b7108-3446-411a-8ebc-642c2b2e6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "df[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(df[['numerical_column1', 'numerical_column2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afe38b-f366-42ca-b011-d56d24454997",
   "metadata": {},
   "source": [
    "8. Split Dataset into Train/Test\n",
    "The cleaned and prepared dataset was split into training and testing sets to allow model validation on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f255ce-c742-49f3-a7af-a36c3ce471cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
