{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2636cb1b-f7ef-40a8-9c11-cb764dbdb2e5",
      "metadata": {
        "id": "2636cb1b-f7ef-40a8-9c11-cb764dbdb2e5"
      },
      "source": [
        " 02 – DATA PREPROCESSING\n",
        "\n",
        "The purpose of this notebook is to clean and prepare the raw data so it’s suitable for analysis and modeling. This step includes loading the dataset, checking for issues like missing values or duplicates, and making sure the data types are correct. It also covers encoding categorical variables, scaling features, and preparing the train/test split."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13c99a5-488a-490a-9910-4cb52abad1f8",
      "metadata": {
        "id": "e13c99a5-488a-490a-9910-4cb52abad1f8"
      },
      "source": [
        "1. Load the Datasets\n",
        "   \n",
        "Instead of loading a single file, this step loads all CSV files located in the data/raw/ folder into a dictionary of pandas DataFrames. Each dataset is stored using its filename (without the .csv extension) as the key. This approach allows easy access and inspection of multiple datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "577de4e1-8c62-40b6-9d93-ab0c405ac994",
      "metadata": {
        "id": "577de4e1-8c62-40b6-9d93-ab0c405ac994",
        "outputId": "262d1d5c-3d87-4751-cd1d-2858517a85c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path exists: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\miche\\\\yacht-data-insights\\\\data\\\\raw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-928d435668f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# List all files in that folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files found in directory:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\miche\\\\yacht-data-insights\\\\data\\\\raw'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Check if the path exists\n",
        "raw_data_path = r'C:\\Users\\miche\\yacht-data-insights\\data\\raw'\n",
        "print(f\"Path exists: {os.path.exists(raw_data_path)}\")\n",
        "\n",
        "# List all files in that folder\n",
        "all_files = os.listdir(raw_data_path)\n",
        "print(\"Files found in directory:\")\n",
        "print(all_files)\n",
        "\n",
        "# List all CSV files\n",
        "csv_files = glob.glob(os.path.join(raw_data_path, '*.csv'))\n",
        "print(\"CSV files detected by glob:\")\n",
        "print(csv_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce4ae79-199d-4407-8a7c-7ff765c2c3b4",
      "metadata": {
        "id": "cce4ae79-199d-4407-8a7c-7ff765c2c3b4"
      },
      "source": [
        "2. Initial Checks: Data Types, Missing Values, and Duplicates\n",
        "This step checks for missing values, incorrect data types, and duplicate records — common issues that need to be addressed before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becd6270-e50d-4b55-aa5b-0d196057956f",
      "metadata": {
        "id": "becd6270-e50d-4b55-aa5b-0d196057956f"
      },
      "outputs": [],
      "source": [
        "# Overview of the dataset\n",
        "df.info()\n",
        "\n",
        "# Count missing values per column\n",
        "df.isnull().sum()\n",
        "\n",
        "# Check for duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c571d596-14c6-4fd6-b6bc-6b34954efad5",
      "metadata": {
        "id": "c571d596-14c6-4fd6-b6bc-6b34954efad5"
      },
      "source": [
        "3. Handle Missing Values\n",
        "Dropped rows with missing values in the target column, since they can’t be used for training. Other missing numerical values were filled using the median to avoid skewing from outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0808ef-67f0-47ec-8d30-744c3de61f61",
      "metadata": {
        "id": "3e0808ef-67f0-47ec-8d30-744c3de61f61"
      },
      "outputs": [],
      "source": [
        "# Drop rows where the target is missing\n",
        "df = df.dropna(subset=['target_column'])\n",
        "\n",
        "# Fill missing values in numerical columns\n",
        "df['numerical_column'] = df['numerical_column'].fillna(df['numerical_column'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ef14ca-e0a5-4ddf-83b8-5f82708e69b1",
      "metadata": {
        "id": "19ef14ca-e0a5-4ddf-83b8-5f82708e69b1"
      },
      "source": [
        "4. Fix Data Types\n",
        "Some columns were not in the correct format for processing. Converting them to the appropriate data types ensures compatibility with future transformations and modeling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14454a8-3da1-4087-b579-44578b3dbf97",
      "metadata": {
        "id": "d14454a8-3da1-4087-b579-44578b3dbf97"
      },
      "outputs": [],
      "source": [
        "# Convert values to numeric\n",
        "df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c331bd89-d996-45de-8a29-bdde2b724737",
      "metadata": {
        "id": "c331bd89-d996-45de-8a29-bdde2b724737"
      },
      "source": [
        "5. Remove Duplicates\n",
        "Removing duplicate rows to avoid bias and redundancy during analysis or training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255dc7e7-62e4-4182-a766-c7bfde6b1dd7",
      "metadata": {
        "id": "255dc7e7-62e4-4182-a766-c7bfde6b1dd7"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9b48fe-f4eb-4c49-8344-2611dd53d118",
      "metadata": {
        "id": "be9b48fe-f4eb-4c49-8344-2611dd53d118"
      },
      "source": [
        "6. Encode Categorical Variables\n",
        "Categorical columns were one-hot encoded to convert them into a numerical format suitable for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b6e2b0-8854-484a-b897-acd0b0d0af40",
      "metadata": {
        "id": "52b6e2b0-8854-484a-b897-acd0b0d0af40"
      },
      "outputs": [],
      "source": [
        "# One-hot encode selected categorical column\n",
        "df = pd.get_dummies(df, columns=['categorical_column'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf51003-4de7-4265-b3e6-d883ae0820b7",
      "metadata": {
        "id": "5bf51003-4de7-4265-b3e6-d883ae0820b7"
      },
      "source": [
        "7. Scale Numerical Features\n",
        "Numerical features were scaled using StandardScaler to ensure all features contribute equally to model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87b7108-3446-411a-8ebc-642c2b2e6a45",
      "metadata": {
        "id": "f87b7108-3446-411a-8ebc-642c2b2e6a45"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "df[['numerical_column1', 'numerical_column2']] = scaler.fit_transform(df[['numerical_column1', 'numerical_column2']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8afe38b-f366-42ca-b011-d56d24454997",
      "metadata": {
        "id": "f8afe38b-f366-42ca-b011-d56d24454997"
      },
      "source": [
        "8. Split Dataset into Train/Test\n",
        "The cleaned and prepared dataset was split into training and testing sets to allow model validation on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f255ce-c742-49f3-a7af-a36c3ce471cb",
      "metadata": {
        "id": "38f255ce-c742-49f3-a7af-a36c3ce471cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Create training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}